---
title: "ğŸ§  Reinforcement Learning: Reborn to Become the CEO"
summary: "Let algorithms survive KPI combat and master the essence of RL through workplace simulations!"
date: 2025-04-07
math: true
authors: 
  - Yixiang Dai
tags: 
  - Reinforcement Learning
  - Educational
  - AI Workplace Simulation
image:
  caption: "Use videos, LaTeX, code, and humor to make RL approachable"
---

# å¼ºåŒ–å­¦ä¹ ï¼šé‡ç”Ÿä¹‹æˆ‘è¦æˆä¸ºCEO
**â€”â€”è®©ç®—æ³•åœ¨KPIæš´å‡»ä¸­å­¦ä¼šèŒåœºç”Ÿå­˜æ³•åˆ™**

---
## ç¬¬ä¸€ç« ï¼šè¿™ä¸ªRLç©¶ç«Ÿæ˜¯ä»€ä¹ˆé¬¼ï¼Ÿ
### æ­£ç»å­¦æœ¯å®šä¹‰
> é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’å­¦ä¹ æœ€ä¼˜å†³ç­–ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±çš„æœºå™¨å­¦ä¹ èŒƒå¼
### 1.1 è‚¤æµ…å®šä¹‰
> "åœ¨è€æ¿çœ¼çš®åº•ä¸‹å·å­¦å‡èŒç§˜ç±çš„ç„å­¦"
- **ç›‘ç£å­¦ä¹ **ï¼šé¢†å¯¼æ‰‹æŠŠæ‰‹æ•™ä½ æ€ä¹ˆå†™PPT
- **æ— ç›‘ç£å­¦ä¹ **ï¼šæ²¡äººç®¡ä½ åœ¨èŒ¶æ°´é—´çç¢ç£¨
- **å¼ºåŒ–å­¦ä¹ **ï¼šæ¯æ¬¡æ–¹æ¡ˆè¢«é©³å›éƒ½å·å·è®°å°æœ¬æœ¬
**<span style="color:red">1. è¾“å…¥çš„æ ·æœ¬æ˜¯åºåˆ—æ•°æ®</span>  
<span style="color:red">2. å¥–åŠ±ä¿¡å·æ˜¯å»¶è¿Ÿçš„ï¼Œå³ç¯å¢ƒä¼šåœ¨å¾ˆä¹…ä»¥åå‘Šè¯‰æˆ‘ä»¬ä¹‹å‰æˆ‘ä»¬é‡‡å–çš„åŠ¨ä½œåˆ°åº•æ˜¯ä¸æ˜¯æœ‰æ•ˆçš„</span>
<span style="color:red">3. å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒåœ¨äºé€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’å­¦ä¹ ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥ï¼Œä»è€Œåœ¨ä¸ç¡®å®šå’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­æœ€å¤§åŒ–é•¿æœŸç´¯è®¡å¥–åŠ±</span>**
---

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦å¼ºåŒ–å­¦ä¹ å‘¢ï¼Ÿ

#### ä¼ ç»Ÿæ–¹æ³•çš„"å›°å¢ƒ"
```python
# ç›‘ç£å­¦ä¹ ã®æ­»æ¿
if ä»»åŠ¡ in çŸ¥è¯†åº“:
    ç…§æŠ„å‰ä»»æ–¹æ¡ˆ
else:
    raise "è¿™é¢˜é¢†å¯¼æ²¡æ•™è¿‡ï¼"

# æ— ç›‘ç£å­¦ä¹ ã®ä½›ç³»
åˆ†ææ‰€æœ‰ä¼šè®®è®°å½• â†’ ç”Ÿæˆè¯äº‘å›¾ â†’ ä¾ç„¶ä¸çŸ¥é“PPTæ€ä¹ˆå†™
```
#### ä¸‰å¤§å­¦æ´¾ã®ç»ˆæå¯¹å†³
| **å­¦ä¹ ç±»å‹**   | **ç›‘ç£å­¦ä¹ **               | **æ— ç›‘ç£å­¦ä¹ **           | **å¼ºåŒ–å­¦ä¹ **                 |
|----------------|---------------------------|-------------------------|-----------------------------|
| **æ•°æ®é¥²æ–™**    | å¸¦æ ‡ç­¾çš„(è¾“å…¥,è¾“å‡º)å¯¹      | æ— æ ‡ç­¾æ•°æ®              | çŠ¶æ€-åŠ¨ä½œ-å¥–åŠ±åºåˆ—          |
| **ç»ˆæç›®æ ‡**    | å¤ç°æ ‡å‡†ç­”æ¡ˆ              | å‘ç°æ•°æ®å†…åœ¨ç»“æ„        | æœ€å¤§åŒ–é•¿æœŸå¥–åŠ±              |
| **åé¦ˆæœºåˆ¶**    | å³æ—¶æ˜ç¡®çš„é”™è¯¯æç¤º        | æ— æ˜ç¡®åé¦ˆ              | å»¶è¿Ÿä¸”ç¨€ç–çš„å¥–åŠ±ä¿¡å·        |
| **äººç±»æ¯”å–»**    | å­¦éœ¸åˆ·äº”å¹´é«˜è€ƒä¸‰å¹´æ¨¡æ‹Ÿ    | è‰ºæœ¯å®¶åœ¨åƒåœ¾å †æ‰¾çµæ„Ÿ    | ç¤¾ç•œåœ¨KPIè¿·é›¾ä¸­æ‘¸çˆ¬æ»šæ‰“      |

---
**<span style="color:red">ç›‘ç£å­¦ä¹ ï¼šæ ‡ç­¾çš„è·å–ä»£ä»·å¾€å¾€è¾ƒä¸ºæ˜‚è´µ</span>**  
**<span style="color:red">å¼ºåŒ–å­¦ä¹ ï¼šæ›´åŠ ç¬¦åˆäººè®¤è¯†ä¸–ç•Œçš„è¿‡ç¨‹</span>**

### 1.3 æ·±å…¥å®šä¹‰


#### 1.3.1 çŠ¶æ€ (State)
- **è¾…åŠ©ç†è§£**ï¼š  
  çŠ¶æ€å°±å¥½æ¯”ä½ åœ¨èŒåœºä¸­çš„å„ä¸ªâ€œèº«ä»½â€ï¼šå¯èƒ½æ˜¯å¿™å¾—å›¢å›¢è½¬çš„æ‰“å·¥äººï¼Œä¹Ÿå¯èƒ½æ˜¯ç»Ÿç­¹å…¨å±€çš„CEOï¼Œæ¯ä¸ªçŠ¶æ€éƒ½åæ˜ äº†å½“å‰ä½ æ‰€å¤„çš„ç¯å¢ƒæƒ…æ™¯ã€‚
- **å®šä¹‰**ï¼š  
  åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒçŠ¶æ€ \( s \in S \) æ˜¯æè¿°ç¯å¢ƒåœ¨æŸä¸€æ—¶åˆ»æ‰€æœ‰å¿…è¦ä¿¡æ¯çš„å˜é‡é›†åˆã€‚çŠ¶æ€å¿…é¡»æ»¡è¶³é©¬å°”å¯å¤«æ€§ï¼Œå³æœªæ¥çš„å†³ç­–åªä¾èµ–äºå½“å‰çŠ¶æ€ï¼Œè€Œä¸è¿‡å»æ— å…³ã€‚

#### 1.3.2 åŠ¨ä½œ (Action)
- **è¾…åŠ©ç†è§£**ï¼š  
  åŠ¨ä½œæ˜¯ä½ åœ¨èŒåœºä¸­èƒ½åšå‡ºçš„é€‰æ‹©â€”â€”æ˜¯åŠ ç­ã€å–å’–å•¡å·æ‡’ï¼Œè¿˜æ˜¯å†’é™©å‘è€æ¿æå‡ºåˆ›æ–°æ–¹æ¡ˆï¼Œæ¯ä¸ªé€‰æ‹©éƒ½å¯èƒ½æ”¹å˜ä½ æ™‹å‡çš„è½¨è¿¹ï¼
- **å®šä¹‰**ï¼š  
  åŠ¨ä½œ \( a \in A \) æ˜¯æ™ºèƒ½ä½“åœ¨ç‰¹å®šçŠ¶æ€ \( s \) ä¸‹å¯ä»¥é‡‡å–çš„æ“ä½œã€‚åŠ¨ä½œé›†åˆ \( A \) åŒ…å«äº†æ‰€æœ‰å¯èƒ½çš„å†³ç­–é€‰é¡¹ï¼Œæ˜¯ç­–ç•¥åˆ¶å®šçš„é‡è¦ä¾æ®ã€‚

#### 1.3.3 ç­–ç•¥ (Policy)
- **è¾…åŠ©ç†è§£**ï¼š  
  ç­–ç•¥å°±åƒä½ çš„èŒåœºç”Ÿå­˜æ³•åˆ™ï¼Œæ˜¯ä½ åœ¨ä¸åŒæƒ…å¢ƒä¸‹é€‰æ‹©â€œåŠ ç­â€ã€â€œæ—©é€€â€æˆ–â€œä¸»åŠ¨è¯·ç¼¨â€çš„æ¦‚ç‡åˆ†å¸ƒã€‚ä¸€ä¸ªå¥½çš„ç­–ç•¥ï¼Œæ—¢è¦å…¼é¡¾æ•ˆç‡ï¼Œä¹Ÿè¦é˜²æ­¢è¢«è€æ¿ç›¯ä¸Šï¼
- **å®šä¹‰**ï¼š  
  ç­–ç•¥ \(\pi(a|s)\) å®šä¹‰äº†åœ¨çŠ¶æ€ \( s \) ä¸‹é€‰æ‹©åŠ¨ä½œ \( a \) çš„æ¦‚ç‡åˆ†å¸ƒã€‚å®ƒå¯ä»¥æ˜¯ç¡®å®šæ€§çš„ï¼ˆæ¯ä¸ªçŠ¶æ€ä¸‹éƒ½æœ‰å”¯ä¸€åŠ¨ä½œï¼‰æˆ–éšæœºæ€§çš„ï¼ˆå­˜åœ¨å¤šä¸ªåŠ¨ä½œçš„é€‰æ‹©æ¦‚ç‡ï¼‰ã€‚

#### 1.3.4 ä»·å€¼å‡½æ•° (Value Function)
- **è¾…åŠ©ç†è§£**ï¼š  
  ä»·å€¼å‡½æ•°ç±»ä¼¼äºä½ å¯¹æœªæ¥æ™‹å‡ã€åŠ è–ªçš„â€œé¢„ä¼°â€ï¼Œå®ƒå‘Šè¯‰ä½ å½“å‰çŠ¶æ€ä¸‹å„ä¸ªå†³ç­–å¯èƒ½å¸¦æ¥çš„å›æŠ¥â€”â€”æ¯”å¦‚å“ªæ¡è·¯èƒ½è®©ä½ æå‰æ‹¿åˆ°å¹´ç»ˆå¥–ï¼Œå“ªæ¡è·¯å¯èƒ½åªæ˜¯å¤šå–æ¯å’–å•¡ã€‚
- **å®šä¹‰**ï¼š  
  - **çŠ¶æ€ä»·å€¼å‡½æ•° \(V^{\pi}(s)\)**ï¼šè¡¨ç¤ºåœ¨**ç­–ç•¥ \(\pi\)** ä¸‹ï¼Œä»çŠ¶æ€ \( s \) å¼€å§‹ï¼Œæœªæ¥ç´¯ç§¯å¥–åŠ±çš„æœŸæœ›å€¼ã€‚  
    $$
V^{\pi}(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0=s\right] = \mathbb{E}_{\pi}\left[r_0 + \sum_{t=1}^{\infty} \gamma^t r_t \mid s_0=s\right]
$$
  - **åŠ¨ä½œä»·å€¼å‡½æ•° \(Q^{\pi}(s,a)\)**ï¼šè¡¨ç¤ºåœ¨çŠ¶æ€ \( s \) ä¸‹é‡‡å–åŠ¨ä½œ \( a \) åï¼Œéµå¾ª**ç­–ç•¥ \(\pi\)** æ‰€è·å¾—çš„æœªæ¥ç´¯è®¡å¥–åŠ±çš„æœŸæœ›å€¼ã€‚  
    $$
Q^{\pi}(s,a) = \mathbb{E}_{\pi}\left[r_{0} + \gamma \sum_{t=1}^{\infty} \gamma^{t-1} r_t \mid s_0=s, a_0=a\right]
$$

#### 1.3.5 ç¯å¢ƒæ¨¡å‹ (Model)
- **è¾…åŠ©ç†è§£**ï¼š  
  æ¨¡å‹å°±åƒæ˜¯ä½ æå‰æ‹¿åˆ°çš„â€œèŒåœºå‰§æœ¬â€ï¼Œæè¿°äº†åœ¨ä½ åšå‡ºæ¯ä¸ªå†³ç­–åï¼Œç¯å¢ƒï¼ˆæˆ–è€æ¿ï¼‰å¦‚ä½•ååº”â€”â€”æ˜¯æ¶¨è–ªè¿˜æ˜¯æ‰£å¥–é‡‘ï¼Œå…¨åœ¨è¿™å‰§æœ¬é‡Œï¼
- **å®šä¹‰**ï¼š  
  ç¯å¢ƒæ¨¡å‹ç”±çŠ¶æ€è½¬ç§»æ¦‚ç‡ \( P(s'|s,a) \) ä¸å¥–åŠ±å‡½æ•° \( R(s,a,s') \) æ„æˆï¼š
  - **çŠ¶æ€è½¬ç§»æ¦‚ç‡**ï¼š\( P(s'|s,a) \) è¡¨ç¤ºåœ¨çŠ¶æ€ \( s \) ä¸‹é‡‡å–åŠ¨ä½œ \( a \) åè½¬ç§»åˆ°çŠ¶æ€ \( s' \) çš„æ¦‚ç‡ã€‚
  - **å¥–åŠ±å‡½æ•°**ï¼š\( R(s,a,s') \) æè¿°äº†çŠ¶æ€ \( s \) ç»è¿‡åŠ¨ä½œ \( a \) è½¬å˜ä¸º \( s' \) åè·å¾—çš„å³æ—¶å¥–åŠ±ã€‚
  
  æ‹¥æœ‰æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯ä»¥åˆ©ç”¨è¿™äº›ä¿¡æ¯è¿›è¡Œé¢„æµ‹å’Œè§„åˆ’ï¼Œè€Œå…æ¨¡å‹æ–¹æ³•åˆ™ç›´æ¥ä¾èµ–äºä¸ç¯å¢ƒçš„äº¤äº’åé¦ˆã€‚

---

## ç¬¬äºŒç« ï¼šRLè¦å­¦ä»€ä¹ˆå‘¢ï¼Ÿ

ğŸ§ : å¼ºåŒ–å­¦ä¹ çš„ç»ˆæç›®æ ‡å…¶å®æ˜¯å­¦åˆ°**æœ€ä¼˜ç­–ç•¥**ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªèƒ½åœ¨å„ç§çŠ¶æ€ä¸‹åšå‡ºæœ€ä½³å†³ç­–çš„æ˜ å°„å‡½æ•°ã€‚ä¸è¿‡ï¼Œä»·å€¼å‡½æ•° \(V\) å’ŒåŠ¨ä½œä»·å€¼å‡½æ•° \(Q\) åœ¨å®ç°è¿™ä¸ªç›®æ ‡æ—¶æ‰®æ¼”äº†éå¸¸é‡è¦çš„ä¸­é—´è§’è‰²ã€‚

### 2.1 çŠ¶æ€ä»·å€¼ã€åŠ¨ä½œä»·å€¼ä¸ç­–ç•¥çš„æ•°å­¦ç»Ÿä¸€æ€§
#### 2.1.1 è´å°”æ›¼æ–¹ç¨‹
$$
\begin{aligned}V(s)&=\mathbb{E}\begin{bmatrix}r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\ldots|s_t=s\end{bmatrix}\\&=\mathbb{E}\left[r_{t+1}|s_{t}=s\right]+\gamma\mathbb{E}\left[r_{t+2}+\gamma r_{t+3}+\gamma^{2}r_{t+4}+\ldots\mid s_{t}=s\right]\\&=R(s)+\gamma\mathbb{E}[V(s_{t+1})|s_t=s]\\&=R(s)+\gamma\sum_{s^{\prime}\in S}P_{\pi}\left(s^{\prime}\mid s\right)V(s^{\prime})\end{aligned}
$$
#### 2.2 ä¸‰ä½ä¸€ä½“
$$
\begin{aligned}
&\boxed{
\begin{aligned}
&1.\ V^\ast(s) = \max_a Q^\ast(s,a) \\
&2.\ Q^\ast(s,a) = \mathbb{E} \left[ r + \gamma V^\ast(s') \right] \\
&3.\ \pi^\ast = \arg\max_a Q^\ast(s,a)
\end{aligned}
}
\end{aligned}
$$
### 2.2.1. çŠ¶æ€ä»·å€¼ä¸åŠ¨ä½œä»·å€¼çš„äº’æ¨


$$
V^{\pi}(s) = \sum_{a \in A} \pi(a|s) Q^{\pi}(s,a)
$$
**è§£è¯»**ï¼šåœ¨ç­–ç•¥Ï€ä¸‹ï¼ŒçŠ¶æ€ä»·å€¼æ˜¯å„åŠ¨ä½œä»·å€¼çš„æ¦‚ç‡åŠ æƒå¹³å‡
**èŒåœºæ˜ å°„**ï¼šä½ çš„æ•´ä½“èº«ä»· = å„ç”Ÿå­˜ç­–ç•¥ï¼ˆæ‹é©¬/å®å¹²/ç”©é”…ï¼‰çš„æœŸæœ›æ”¶ç›Š

### åº”ç”¨åˆ°å¼ºåŒ–å­¦ä¹ ä¸­

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å…³å¿ƒçš„æ˜¯çŠ¶æ€ \(s\) ä¸‹çš„ç´¯è®¡å›æŠ¥ã€‚å‡è®¾åœ¨çŠ¶æ€ \(s\) æ—¶ï¼Œæˆ‘ä»¬çš„ç­–ç•¥ \(\pi\) å®šä¹‰äº†é€‰æ‹©å„ä¸ªåŠ¨ä½œ \(a\) çš„æ¦‚ç‡ \(\pi(a|s)\)ã€‚é‚£ä¹ˆåœ¨çŠ¶æ€ \(s\) ä¸‹ï¼Œç´¯è®¡å›æŠ¥ï¼ˆä¹Ÿå°±æ˜¯çŠ¶æ€ä»·å€¼å‡½æ•° \(V^\pi(s)\)ï¼‰å¯ä»¥å†™ä½œå¯¹ä¸åŒåŠ¨ä½œå¸¦æ¥çš„å›æŠ¥çš„æ¡ä»¶æœŸæœ›çš„åŠ æƒå¹³å‡ï¼š
\[
V^\pi(s) = \mathbb{E}\left[\text{ç´¯è®¡å›æŠ¥} \mid s_0 = s\right].
\]

åˆ©ç”¨å…¨æ¦‚ç‡å…¬å¼ï¼Œå°†â€œå…ˆé€‰æ‹©åŠ¨ä½œï¼Œå†è€ƒè™‘å¯¹åº”å›æŠ¥â€çš„è¿‡ç¨‹å±•å¼€ï¼š
\[
V^\pi(s) = \sum_{a\in A} \mathbb{E}\left[\text{ç´¯è®¡å›æŠ¥} \mid s_0=s, a_0=a\right] \, P(a_0=a \mid s_0=s).
\]

æ³¨æ„ï¼š
- \(P(a_0=a \mid s_0=s)\) æ­£æ˜¯ç­–ç•¥ \(\pi(a|s)\)ã€‚
- \(\mathbb{E}\left[\text{ç´¯è®¡å›æŠ¥} \mid s_0=s, a_0=a\right]\) å°±æ˜¯åŠ¨ä½œä»·å€¼å‡½æ•° \(Q^\pi(s,a)\)ã€‚

å› æ­¤ï¼Œåˆ©ç”¨æ¡ä»¶æ¦‚ç‡å’Œå…¨æ¦‚ç‡å…¬å¼ï¼Œæˆ‘ä»¬æœ‰ï¼š
\[
V^\pi(s) = \sum_{a\in A} \pi(a|s) \, Q^\pi(s,a).
\]
#### å·²çŸ¥æœ€ä¼˜Qæ¨å¯¼æœ€ä¼˜V
$$
V^*(s) = \max_a Q^*(s,a)
$$

**è¯æ˜**ï¼š
æ ¹æ®å®šä¹‰ï¼Œæœ€ä¼˜ç­–ç•¥ä¸‹åªé€‰æ‹©æœ€å¤§Qå€¼çš„åŠ¨ä½œ
æ­¤æ—¶ç­–ç•¥æ˜¯ç¡®å®šæ€§åˆ†å¸ƒï¼š
$$
\pi^*(a|s) = \begin{cases}1 & a = \arg\max Q^* \\ 0 & \text{å…¶ä»–}\end{cases}
$$ 
ä»£å…¥Vçš„å®šä¹‰å¼ï¼š
$$
V^*(s) = \sum_a \pi^*(a|s)Q^*(s,a) = \max_a Q^*(s,a)
$$


#### å·²çŸ¥æœ€ä¼˜Væ¨å¯¼æœ€ä¼˜Q
$$
Q^*(s,a) = \sum_{s'} P(s'|s,a)\left[ R(s,a,s') + \gamma V^*(s') \right]
$$
è¯æ˜ï¼š
æ ¹æ®è´å°”æ›¼æ–¹ç¨‹å¯¹Qçš„å®šä¹‰ï¼Œæœ€ä¼˜Qåº”æ»¡è¶³ï¼š

$$
Q^*(s,a) = \mathbb{E}\left[ r + \gamma V^*(s') \right]
$$

è€Œæœ€ä¼˜V*åˆæ»¡è¶³ï¼š
$$
V^*(s') = \max_{a'} Q^*(s',a')
$$


#### å·²çŸ¥æœ€ä¼˜Qæ¨å¯¼æœ€ä¼˜ç­–ç•¥
$$
\pi^\ast = \arg\max_a Q^\ast(s,a)
$$











---
## ç¬¬ä¸‰ç« ï¼šå¦‚ä½•å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Ÿ
## å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ ‘çŠ¶å›¾

```mermaid
graph TD
    A[å¼ºåŒ–å­¦ä¹ ] --> B[æœ‰æ¨¡å‹ Model-Based]
    A --> C[å…æ¨¡å‹ Model-Free]
    
    %% åŠ¨æ€è§„åˆ’ (æœ‰æ¨¡å‹)
    B --> D[åŠ¨æ€è§„åˆ’ Dynamic Programming]
    
    %% åŠ¨æ€è§„åˆ’ä¸‹: é¢„æµ‹ & æ§åˆ¶
    D --> D_Pred[ç­–ç•¥è¯„ä¼° é¢„æµ‹ Policy Evaluation]
    D --> D_Ctrl[æ§åˆ¶ Control]
    
    %% ä¸¤ç§æ§åˆ¶æ–¹æ³•: ä»·å€¼è¿­ä»£ & ç­–ç•¥è¿­ä»£
    D_Ctrl --> F[ä»·å€¼è¿­ä»£ Value Iteration]
    D_Ctrl --> G[ç­–ç•¥è¿­ä»£ Policy Iteration]
    
    %% å…æ¨¡å‹ä¸‹ï¼šè’™ç‰¹å¡æ´› & æ—¶åºå·®åˆ†
    C --> K[è’™ç‰¹å¡æ´› Monte Carlo]
    C --> J[æ—¶åºå·®åˆ† Temporal Difference]
    
    %% è’™ç‰¹å¡æ´›åˆ†æˆé¢„æµ‹ & æ§åˆ¶
    K --> K_Pred[é¢„æµ‹ MC Prediction]
    K --> K_Ctrl[æ§åˆ¶ MC Control]
    
    %% æ—¶åºå·®åˆ†åˆ†æˆé¢„æµ‹ & æ§åˆ¶
    J --> J_Pred[é¢„æµ‹ TD Prediction]
    J --> J_Ctrl[æ§åˆ¶ SARSA, Q-Learning, etc.]


```

## 1. ä»€ä¹ˆæ˜¯æœ‰æ¨¡å‹ï¼ˆModel-Basedï¼‰ï¼Ÿ

- **å®šä¹‰ï¼š**  
  æœ‰æ¨¡å‹çš„æ–¹æ³•éœ€è¦çŸ¥é“æˆ–è€…å­¦ä¹ ç¯å¢ƒçš„â€œå†…å¹•æ¶ˆæ¯â€â€”â€”ä¹Ÿå°±æ˜¯çŠ¶æ€è½¬ç§»æ¦‚ç‡ \( p(s' \mid s, a) \) å’Œå¥–åŠ±å‡½æ•° \( R(s,a,s') \)ã€‚

- **å·¥ä½œåŸç†ï¼š**  
  - **å·²çŸ¥æ¨¡å‹ï¼š**  
    å¦‚æœç¯å¢ƒè§„åˆ™æ‘†åœ¨é‚£å„¿ï¼Œç›´æ¥åˆ©ç”¨åŠ¨æ€è§„åˆ’ï¼ˆæ¯”å¦‚ä»·å€¼è¿­ä»£ã€ç­–ç•¥è¿­ä»£ï¼‰æå®šé—®é¢˜ã€‚
  - **å­¦æ¨¡å‹ï¼š**  
    å¦‚æœæ²¡æœ‰å‰§æœ¬ï¼Œå°±å¾—è‡ªå·±æä¸ªè¿‘ä¼¼æ¨¡å‹ï¼Œé€šè¿‡é‡‡æ ·æˆ–äº¤äº’æ¥å‡‘åˆç”¨ã€‚

- **ä¼˜ç‚¹ä¸ç¼ºç‚¹ï¼š**  
  - **ä¼˜ç‚¹ï¼š** å¯ä»¥æå‰åšè¶³â€œé¢„æµ‹â€ï¼Œè§„åˆ’æœªæ¥ï¼Œå ªæ¯”ã€Šæœªæ¥æœºå™¨ã€‹é‡Œçš„é¢„è¨€å®¶ã€‚  
  - **ç¼ºç‚¹ï¼š** ææ¸…æ¥šç¯å¢ƒçš„æ‰€æœ‰ç»†èŠ‚æœ‰æ—¶å€™æ¯”å¼„æ‡‚ã€ŠæƒåŠ›çš„æ¸¸æˆã€‹é‡Œçš„å®¶æ—å…³ç³»è¿˜éš¾ï¼

---

## 2. ä»€ä¹ˆæ˜¯å…æ¨¡å‹ï¼ˆModel-Freeï¼‰ï¼Ÿ

- **å®šä¹‰ï¼š**  
  å…æ¨¡å‹çš„æ–¹æ³•å°±æ˜¯â€œèµ°ç€çå­¦â€ï¼Œå®Œå…¨ä¸ä¾èµ–é‚£ä¸ªçƒ¦äººçš„ç¯å¢ƒæ¨¡å‹ï¼Œç›´æ¥å’Œç¯å¢ƒæ„‰å¿«äº’åŠ¨ï¼Œé å®é™…è§‚æµ‹çš„æ•°æ®æ¥æ›´æ–°ç­–ç•¥æˆ–ä»·å€¼å‡½æ•°ã€‚

- **å·¥ä½œåŸç†ï¼š**  
  - å°±åƒæ—¶åºå·®åˆ†å­¦ä¹ å’ŒQå­¦ä¹ ä¸€æ ·ï¼Œç›´æ¥ç”¨ä½ ä»ç¯å¢ƒé‚£å„¿æ”¶åˆ°çš„å³æ—¶å¥–åŠ± \( r_{t+1} \) å’Œä¸‹ä¸€ä¸ªçŠ¶æ€ \( s_{t+1} \) æ¥è°ƒæ•´å½“å‰ä¼°è®¡ï¼Œè€Œä¸ç®¡æ¨¡å‹é•¿å•¥æ ·ã€‚

- **ä¼˜ç‚¹ä¸ç¼ºç‚¹ï¼š**  
  - **ä¼˜ç‚¹ï¼š** å®ç°ç®€å•ï¼Œä¸éœ€è¦èƒŒé‚£ä¹ˆå¤šâ€œå‰§æœ¬â€ï¼Œç‰¹åˆ«é€‚åˆç¯å¢ƒå¤ªå¤æ‚ã€æ¨¡å‹å¤ªéšç§˜çš„æƒ…å†µã€‚  
  - **ç¼ºç‚¹ï¼š** å¯èƒ½éœ€è¦å¤§é‡æ•°æ®ï¼Œæ ·æœ¬æ•ˆç‡æœ‰æ—¶å€™æ¯”çƒ¤å…¨ç¾Šè¿˜è¦æ…¢ï¼

---

## 3. ä»€ä¹ˆæ˜¯é¢„æµ‹ (Prediction)ï¼Ÿ

- **å®šä¹‰ï¼š**  
  é¢„æµ‹ï¼Œä¹Ÿç§°ä¸ºâ€œè¯„ä¼°â€ï¼ŒæŒ‡çš„æ˜¯åœ¨ç»™å®šæŸä¸ªç­–ç•¥ \(\pi\) çš„æƒ…å†µä¸‹ï¼Œä¼°è®¡æ¯ä¸ªçŠ¶æ€ï¼ˆæˆ–çŠ¶æ€-åŠ¨ä½œå¯¹ï¼‰çš„ä»·å€¼ã€‚æ¢å¥è¯è¯´ï¼Œå°±æ˜¯å‘Šè¯‰ä½ â€œå¦‚æœè€è€å®å®æŒ‰è¿™ä¸ªç­–ç•¥èµ°ï¼Œä»æŸä¸ªçŠ¶æ€å‡ºå‘ï¼Œæœªæ¥èƒ½è·å¾—å¤šå°‘å¥–åŠ±â€ã€‚

- **å·¥ä½œåŸç†ï¼š**  
  - **ç›®æ ‡ï¼š** å­¦ä¹ ä»·å€¼å‡½æ•° \( V^\pi(s) \) æˆ– \( Q^\pi(s, a) \)ï¼Œç”¨äºè¡¡é‡æ¯ä¸ªçŠ¶æ€æˆ–åŠ¨ä½œçš„â€œå¥½åâ€ã€‚  
  - **æ–¹æ³•ï¼š**  
    - **æœ‰æ¨¡å‹é¢„æµ‹ï¼š** åˆ©ç”¨å·²çŸ¥çš„ç¯å¢ƒæ¨¡å‹ \( p(s' \mid s, a) \) å’Œå¥–åŠ±å‡½æ•° \( R(s,a,s') \)ï¼Œé€šè¿‡åŠ¨æ€è§„åˆ’ï¼ˆæ¯”å¦‚æ”¿ç­–è¯„ä¼°ï¼‰æ¥è®¡ç®—ä»·å€¼å‡½æ•°ã€‚  
    - **å…æ¨¡å‹é¢„æµ‹ï¼š** é€šè¿‡ä¸ç¯å¢ƒäº’åŠ¨ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•æˆ–æ—¶åºå·®åˆ†ï¼ˆTDï¼‰å­¦ä¹ ï¼Œä»ç»éªŒæ•°æ®ä¸­é€æ­¥é€¼è¿‘çœŸå®ä»·å€¼ã€‚

- **ä¼˜ç‚¹ä¸ç¼ºç‚¹ï¼š**  
  - **ä¼˜ç‚¹ï¼š** èƒ½å¤Ÿå¯¹ç­–ç•¥çš„é•¿æœŸè¡¨ç°åšå‡ºåˆç†è¯„ä¼°ï¼Œå°±åƒä¸€ä¸ªç»éªŒä¸°å¯Œçš„é¢„è¨€å®¶ï¼Œå‘Šè¯‰ä½ æœªæ¥çš„å¥½åã€‚  
  - **ç¼ºç‚¹ï¼š** å¦‚æœç­–ç•¥ä¸å¥½ï¼Œå³ä½¿é¢„æµ‹å¾—å†å‡†ç¡®ï¼Œæœªæ¥ä¹Ÿä¾ç„¶æ˜¯â€œæƒ¨æ·¡ç»è¥â€ï¼›å¦å¤–ï¼Œå…æ¨¡å‹é¢„æµ‹å¾€å¾€éœ€è¦å¤§é‡æ•°æ®ï¼Œè®­ç»ƒè¿‡ç¨‹å¯èƒ½ä¼šæ¯”è¾ƒæ…¢ã€‚

---

## 4. ä»€ä¹ˆæ˜¯æ§åˆ¶ (Control)ï¼Ÿ

- **å®šä¹‰ï¼š**  
  æ§åˆ¶æŒ‡çš„æ˜¯åœ¨è¯„ä¼°çš„åŸºç¡€ä¸Šï¼Œ**å¯»æ‰¾æœ€ä¼˜ç­–ç•¥**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œé€šè¿‡ä¸æ–­è¯•é”™ã€æ”¹è¿›ï¼Œæœ€ç»ˆæ‰¾åˆ°ä¸€å¥—èƒ½ä½¿ç´¯è®¡å¥–åŠ±æœ€å¤§çš„å†³ç­–æ–¹æ¡ˆã€‚  
  - æ§åˆ¶ä¸ä»…è¦æ±‚ä½ çŸ¥é“â€œå“ªä¸ªçŠ¶æ€å¥½â€ï¼Œè¿˜å¾—å‘Šè¯‰ä½ â€œè¯¥å¹²å˜›â€â€”â€”å…·ä½“å“ªä¸€æ­¥èµ°æ‰èƒ½æŠŠå±€é¢å˜å¾—æ›´æœ‰åˆ©ã€‚

- **å·¥ä½œåŸç†ï¼š**  
  - **ç›®æ ‡ï¼š** ç›´æ¥æˆ–é—´æ¥åœ°å­¦ä¹ æœ€ä¼˜ä»·å€¼å‡½æ•° \( V^*(s) \) æˆ–æœ€ä¼˜åŠ¨ä½œä»·å€¼å‡½æ•° \( Q^*(s,a) \)ï¼Œå¹¶ç”±æ­¤ç¡®å®šæœ€ä¼˜ç­–ç•¥ \(\pi^*\)ã€‚  
  - **æ–¹æ³•ï¼š**  
    - **æœ‰æ¨¡å‹æ§åˆ¶ï¼š** åˆ©ç”¨ç¯å¢ƒæ¨¡å‹ï¼Œé€šè¿‡ç­–ç•¥è¿­ä»£æˆ–ä»·å€¼è¿­ä»£ç­‰æ–¹æ³•è¿›è¡Œè§„åˆ’å’Œå†³ç­–ã€‚  
    - **å…æ¨¡å‹æ§åˆ¶ï¼š** é€šè¿‡ç›´æ¥ä¸ç¯å¢ƒäº¤äº’ï¼Œé‡‡ç”¨ Q-learningã€SARSAã€Actor-Critic ç­‰æ–¹æ³•ï¼Œåœ¨è¯•é”™ä¸­é€æ­¥æ”¹è¿›ç­–ç•¥ï¼Œæœ€ç»ˆè·å¾—æœ€ä¼˜å†³ç­–ã€‚
    
- **ä¼˜ç‚¹ä¸ç¼ºç‚¹ï¼š**  
  - **ä¼˜ç‚¹ï¼š** æ§åˆ¶æ–¹æ³•ç›´æ¥å…³æ³¨å¦‚ä½•åšå†³ç­–ï¼Œç›¸è¾ƒäºå•çº¯é¢„æµ‹ï¼Œå®ƒèƒ½å¤Ÿä¸æ–­ä¼˜åŒ–ï¼Œæ‰¾åˆ°é‚£æ¡é€šå¾€â€œæˆåŠŸâ€çš„æœ€ä½³è·¯å¾„ï¼Œå°±åƒä»â€œå›šå¾’å›°å¢ƒâ€ä¸­æ‰¾åˆ°äº†å‡ºè·¯ã€‚  
  - **ç¼ºç‚¹ï¼š** è¿‡ç¨‹å¯èƒ½éå¸¸ä¾èµ–å¤§é‡çš„è¯•éªŒæ•°æ®å’Œæ¢ç´¢ç­–ç•¥ï¼Œæ ·æœ¬æ•ˆç‡è¾ƒä½ï¼Œæœ‰æ—¶å€™æ”¹è¿›å¾—åƒçˆ¬å±±ä¸€æ ·æ…¢ï¼ˆéœ€è¦ä¸æ–­å¾€ä¸Šè¯•ï¼Œé˜²æ­¢èµ°åï¼‰ã€‚



---

### ğŸŒ° ç°å®æ¡ˆä¾‹
1. **è‡ªåŠ¨é©¾é©¶**  
   - **é¢„æµ‹**ï¼šè¯„ä¼°"ç¤¼è®©è¡Œäººç­–ç•¥"çš„é€šè¡Œæ•ˆç‡  
   - **æ§åˆ¶**ï¼šå¯»æ‰¾"ç¤¼è®©è¡Œäºº+å˜é“è¶…è½¦"çš„æœ€ä¼˜ç»„åˆç­–ç•¥

2. **æ¸¸æˆAI**  
   - **é¢„æµ‹**ï¼šè®¡ç®—"çŒ¥çå‘è‚²æµ"çš„èƒœç‡  
   - **æ§åˆ¶**ï¼šè¿›åŒ–å‡º"çŒ¥çå‘è‚²+ç²¾å‡†å·å¡”"çš„å† å†›ç­–ç•¥

---


### 3.1 åŠ¨æ€è§„åˆ’ä¹‹ä»·å€¼è¿­ä»£ï¼ˆæœ‰æ¨¡å‹ + æ§åˆ¶ï¼‰
**å®šä¹‰ï¼š**  
ä»·å€¼è¿­ä»£æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„"èŒåœºå·ç‹ç»ˆææŒ‡å—"â€”â€”åœ¨å·²çŸ¥å…¬å¸æ‰€æœ‰æ™‹å‡è§„åˆ™ï¼ˆç¯å¢ƒæ¨¡å‹ï¼‰çš„æƒ…å†µä¸‹ï¼Œç›´æ¥ç®—å‡ºçˆ¬åˆ°CEOä½ç½®çš„æœ€ä¼˜è·¯å¾„ã€‚

**æ ¸å¿ƒå…¬å¼ï¼š**  
\[
V_{k+1}(s) = \max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V_k(s') \right]
\]  
ï¼ˆç¿»è¯‘ï¼šä½ çš„èº«ä»· = å½“å‰åŠ¨ä½œæ”¶ç›Š + æœªæ¥å¯èƒ½èŒä½çš„æœ€å¤§æŠ˜ç°ä»·å€¼ï¼‰

**æ“ä½œæ­¥éª¤ï¼š**
1. **åˆå§‹åŒ–**ï¼šæ‰€æœ‰å²—ä½ä»·å€¼è®¾ä¸º0ï¼ˆå®ä¹ ç”Ÿèµ·æ­¥ä»·ï¼‰
2. **è¿­ä»£å‡çº§**ï¼š  
   - è®¡ç®—æ¯ä¸ªå²—ä½ï¼ˆçŠ¶æ€ï¼‰é€‰æ‹©ä¸åŒåŠ¨ä½œï¼ˆæ‹é©¬å±/åŠ ç­/è·³æ§½ï¼‰åçš„é¢„æœŸèº«ä»·
   - å§‹ç»ˆé€‰æ‹©æœ€é«˜ä»·å€¼çš„æ™‹å‡è·¯å¾„
3. **ç­–ç•¥æå–**ï¼šå½“ä»·å€¼ç¨³å®šåï¼Œæ¯ä¸ªå²—ä½çš„æœ€ä¼˜åŠ¨ä½œå°±æ˜¯é€šå¾€CEOçš„ç§˜ç±

### 3.2 åŠ¨æ€è§„åˆ’ä¹‹ç­–ç•¥è¯„ä¼°ä¸ç­–ç•¥è¿­ä»£ï¼ˆæœ‰æ¨¡å‹ - ï¼ˆé¢„æµ‹+æ§åˆ¶ï¼‰ï¼‰
**å®šä¹‰ï¼š**  
ç­–ç•¥è¿­ä»£æ–¹æ³•åƒæ˜¯åœ¨å…¬å¸é‡Œå®šæœŸè¿›è¡Œç»©æ•ˆè¯„ä¼°å’Œæ™‹å‡è€ƒæ ¸â€”â€”å…ˆè¯„ä¼°ä½ å½“å‰çš„è¡¨ç°ï¼ˆç­–ç•¥è¯„ä¼°ï¼‰ï¼Œå†æ ¹æ®è¯„ä¼°ç»“æœåˆ¶å®šæ™‹å‡è®¡åˆ’ï¼ˆç­–ç•¥æ”¹è¿›ï¼‰ï¼Œä¸æ–­å¾ªç¯ç›´åˆ°è¾¾æˆæœ€ä½³èŒåœºç”Ÿæ¶¯ã€‚

**æ ¸å¿ƒå…¬å¼ï¼š**  
- **ç­–ç•¥è¯„ä¼°ï¼š**  
  å¯¹äºç»™å®šç­–ç•¥ \(\pi\)ï¼Œå®ƒçš„çŠ¶æ€ä»·å€¼å‡½æ•°æ»¡è¶³ï¼š
  \[
  V^{\pi}(s) = \sum_{s'} P(s'|s,\pi(s)) \left[ R(s,\pi(s)) + \gamma V^{\pi}(s') \right]
  \]
- **ç­–ç•¥æ”¹è¿›ï¼š**  
  æ–°ç­–ç•¥ç”±ï¼š
  \[
  \pi'(s) = \arg\max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^{\pi}(s') \right]
  \]
  å®šä¹‰ï¼ˆç¿»è¯‘ï¼šé€‰æ‹©èƒ½è®©ä½ æœªæ¥å‰é€”æœ€å…‰æ˜çš„é‚£æ¡æ™‹å‡è·¯å¾„ï¼‰ã€‚

**æ“ä½œæ­¥éª¤ï¼š**
1. **åˆå§‹åŒ–**ï¼šä»ä¸€ä¸ªéšä¾¿çš„æ™‹å‡ç­–ç•¥å¼€å§‹ï¼ˆä¾‹å¦‚ï¼šæ€»çˆ±åŠ ç­ï¼Œä½†å¶å°”æ‹é©¬å±ï¼‰ã€‚
2. **ç­–ç•¥è¯„ä¼°**ï¼šæ ¹æ®ç°æœ‰æ™‹å‡ç­–ç•¥è®¡ç®—å„å²—ä½çš„é•¿æœŸä»·å€¼ã€‚
3. **ç­–ç•¥æ”¹è¿›**ï¼šæ›´æ–°ç­–ç•¥ï¼Œé€‰æ‹©è®©é•¿æœŸä»·å€¼æœ€å¤§çš„æ™‹å‡åŠ¨ä½œã€‚
4. **å¾ªç¯è¿­ä»£**ï¼šé‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œç›´åˆ°æ™‹å‡è®¡åˆ’ç¨³å®šä¸å˜ï¼Œæ­¤æ—¶ä½ å°±è·å¾—äº†æœ€ä½³æ™‹å‡ç­–ç•¥ï¼Œä¹Ÿå°±æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚

---

## 3.3 è’™ç‰¹å¡æ´›ï¼ˆå…æ¨¡å‹ - ï¼ˆé¢„æµ‹ï¼‰ï¼‰
**å®šä¹‰ï¼š**  
è’™ç‰¹å¡æ´›æ–¹æ³•å°±åƒå‚åŠ ä¸€åœºå¤§å‹èµ°ç§€ï¼Œä½ ä¸æå‰çŸ¥é“æœ€ä½³æœè£…æ­é…ï¼ˆç¯å¢ƒæ¨¡å‹ï¼‰ï¼Œè€Œæ˜¯é€šè¿‡å¤šæ¬¡è¯•ç©¿ã€å…¨ç¨‹èµ°ç§€ï¼ˆå®Œæ•´å›åˆï¼‰ï¼Œæœ€ç»ˆè¯„å‡ºå“ªå¥—é€ å‹æœ€èƒ½èµšå–å›å¤´ç‡ï¼ˆç´¯è®¡å¥–åŠ±ï¼‰ã€‚

**æ ¸å¿ƒæ€æƒ³ï¼š**  
- **å®Œæ•´ä½“éªŒ**ï¼šæ¯æ¬¡èµ°ç§€ï¼ˆå›åˆï¼‰åï¼Œè®°å½•æ•´åœºè¡¨ç°ï¼ˆæ€»å›æŠ¥ï¼‰ã€‚
- **ç»Ÿè®¡å¹³å‡**ï¼šç»è¿‡å¤šæ¬¡è¯•ç©¿ï¼Œè®¡ç®—å‡ºæ¯å¥—æ­é…åœ¨å„ä¸ªåœºåˆï¼ˆçŠ¶æ€ï¼‰çš„å¹³å‡è¡¨ç°ï¼Œå³ä¸ºå…¶ä»·å€¼è¯„ä¼°ã€‚

**æ“ä½œæ­¥éª¤ï¼š**
1. **å¤šæ¬¡è¯•ç©¿**ï¼šä»å½“å‰çŠ¶æ€å¼€å§‹èµ°å®Œæ•´åœºç§€ï¼Œæ¯æ¬¡è¯•ç©¿ä¸åŒæœè£…æ­é…ï¼ˆåŠ¨ä½œï¼‰ã€‚
2. **è®°å½•å›æŠ¥**ï¼šæ¯æ¬¡ç§€åï¼Œè®°å½•ä»å½“å‰çŠ¶æ€åˆ°ç§€ç»ˆï¼ˆå›åˆç»“æŸï¼‰çš„æ•´ä½“å¾—åˆ†ã€‚
3. **è®¡ç®—å¹³å‡**ï¼šå¤šæ¬¡èµ°ç§€åï¼Œå¹³å‡æ¯ä¸ªæ­é…çš„è¡¨ç°ï¼Œå¾—å‡ºä»·å€¼ä¼°è®¡ \(V(s)\)ã€‚

---

## 3.4 å·®åˆ†æ—¶é—´ï¼ˆå…æ¨¡å‹ - ï¼ˆé¢„æµ‹ï¼‰ï¼‰
**å®šä¹‰ï¼š**  
å·®åˆ†æ—¶é—´ï¼ˆTemporal Difference, TDï¼‰æ–¹æ³•å°±åƒåœ¨ä½ çš„æ—¥å¸¸ç”Ÿæ´»ä¸­ï¼Œä¸å¿…ç­‰åˆ°å¹´ç»ˆæ€»ç»“æ‰çŸ¥é“è‡ªå·±æ¶¨è–ªäº†å¤šå°‘ï¼Œè€Œæ˜¯åœ¨æ¯å¤©çš„å·¥ä½œä¸­å³æ—¶æ”¶åˆ°åé¦ˆï¼Œæ ¹æ®ä»Šå¤©çš„è¡¨ç°è°ƒæ•´æ˜å¤©çš„ç›®æ ‡ã€‚

**æ ¸å¿ƒå…¬å¼ï¼ˆTD(0)ï¼‰ï¼š**  
\[
V(s) \leftarrow V(s) + \alpha \Big[ r + \gamma V(s') - V(s) \Big]
\]  
ï¼ˆç¿»è¯‘ï¼šä»Šå¤©çš„è‡ªæˆ‘è¯„ä»· = åŸæœ‰æ°´å¹³ + å­¦ä¹ ç‡ \(\alpha\) Ã—ï¼ˆå³æ—¶å¥–åŠ± + æ˜å¤©é¢„æœŸä»·å€¼æŠ˜ç° - åŸæœ‰æ°´å¹³ï¼‰ï¼‰

**æ“ä½œæ­¥éª¤ï¼š**
1. **å³æ—¶åé¦ˆ**ï¼šæ¯å®Œæˆä¸€æ­¥ï¼ˆçŠ¶æ€è½¬ç§»ï¼‰ï¼Œæ ¹æ®å½“å¤©çš„è¡¨ç° \(r\) å’Œå¯¹æ˜å¤©çš„é¢„æœŸ \(V(s')\) è°ƒæ•´å½“å‰è¯„ä»· \(V(s)\)ã€‚
2. **ä¸æ–­è¿­ä»£**ï¼šéšç€æ—¥ç§¯æœˆç´¯ï¼Œä½ çš„è¯„ä»·ä¼šé€æ¸æ¥è¿‘çœŸå®æ°´å¹³ï¼ˆæœ€ç»ˆæ”¶æ•›åˆ° \(V^*(s)\)ï¼‰ã€‚

---

## 3.5 è’™ç‰¹å¡æ´›ï¼ˆå…æ¨¡å‹ - ï¼ˆæ§åˆ¶ï¼‰ï¼‰
**å®šä¹‰ï¼š**  
è’™ç‰¹å¡æ´›æ§åˆ¶æ–¹æ³•ç±»ä¼¼äºåœ¨èµ°ç§€ä¸­ä¸ä»…è¯„åˆ¤é€ å‹ï¼Œè¿˜æ ¹æ®æ¯æ¬¡èµ°ç§€çš„æ•´ä½“è¡¨ç°è°ƒæ•´æœè£…æ­é…ç­–ç•¥ï¼Œæœ€ç»ˆæ‰¾åˆ°æœ€èƒ½å¸å¼•é•œå¤´çš„æœ€ä½³æ­é…æ–¹æ¡ˆã€‚

**æ ¸å¿ƒæ€æƒ³ï¼š**  
- **è¯•ç©¿ä¸è¯„ä¼°**ï¼šåœ¨æ¯æ¬¡å®Œæ•´èµ°ç§€ï¼ˆå›åˆï¼‰åï¼Œæ ¹æ®è¡¨ç°è°ƒæ•´é€‰æ‹©æœè£…çš„ç­–ç•¥ï¼ˆåŠ¨ä½œï¼‰ã€‚
- **ç­–ç•¥æ›´æ–°**ï¼šé€šè¿‡ä¸æ–­è¯•éªŒä¸åé¦ˆï¼Œæ‰¾åˆ°åœ¨å„ä¸ªåœºåˆä¸­éƒ½èƒ½æœ€å¤§åŒ–å›å¤´ç‡ï¼ˆç´¯è®¡å¥–åŠ±ï¼‰çš„æœè£…æ­é…ç­–ç•¥ã€‚

**æ“ä½œæ­¥éª¤ï¼š**
1. **å¤šæ¬¡å…¨ç¨‹è¯•ç©¿**ï¼šåœ¨æ¯æ¬¡èµ°ç§€ä¸­ï¼Œå°è¯•ä¸åŒçš„æ­é…ç»„åˆã€‚
2. **è®°å½•æ•´ä½“è¡¨ç°**ï¼šæ¯æ¬¡ç§€åï¼Œæ ¹æ®æ•´ä½“å¾—åˆ†è°ƒæ•´å¯¹åº”æ­é…çš„ä»·å€¼è¯„ä¼°ã€‚
3. **ç­–ç•¥æ”¹è¿›**ï¼šé€æ¸å€¾å‘äºé€‰æ‹©é‚£äº›å†å²è¡¨ç°æœ€ä¼˜çš„æ­é…ï¼Œä»è€Œè¾¾åˆ°æœ€ä¼˜æ§åˆ¶ã€‚

---

## 3.6 Q-learning / SARSAï¼ˆå…æ¨¡å‹ - ï¼ˆæ§åˆ¶ï¼‰ï¼‰
**å®šä¹‰ï¼š**  
Q-learning å’Œ SARSA æ˜¯å…æ¨¡å‹æ§åˆ¶ä¸­çš„ä¸¤å¤§ç‹ç‰Œï¼Œå°±åƒä½ åœ¨èŒåœºæ‘¸ç´¢æœ€ä½³æ™‹å‡ç­–ç•¥æ—¶ï¼Œé€šè¿‡ä¸æ–­è¯•é”™æ€»ç»“å‡ºå“ªç§åšæ³•èƒ½è®©è€æ¿åˆ®ç›®ç›¸çœ‹ã€‚  
- **Q-learning** æ˜¯â€œç¦»ç­–ç•¥â€æ–¹æ³•ï¼šå®ƒæ€»æ˜¯ç›¯ç€è€æ¿æœ€å–œæ¬¢çš„é‚£æ¡æ™‹å‡è·¯çº¿ï¼Œä¸ç®¡ä½ å¹³æ—¶æ€ä¹ˆæ··ã€‚  
- **SARSA** æ˜¯â€œåœ¨ç­–ç•¥â€æ–¹æ³•ï¼šå®ƒä¾æ®ä½ å½“å‰çš„å®é™…è¡¨ç°ï¼Œé€æ­¥è°ƒæ•´æ™‹å‡ç­–ç•¥ã€‚

**æ ¸å¿ƒå…¬å¼ï¼š**  
- **Q-learningï¼ˆoff policyï¼‰ï¼š**  
  \[
  Q(s,a) \leftarrow Q(s,a) + \alpha \Big[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \Big]
  \]
  ï¼ˆç¿»è¯‘ï¼šæ›´æ–°å½“å‰æ™‹å‡åŠ¨ä½œçš„ä»·å€¼ = å½“å‰ä¼°è®¡ + å­¦ä¹ ç‡ Ã—ï¼ˆå³æ—¶å¥–åŠ± + ä¸‹ä¸ªå²—ä½æœ€ä½³æ™‹å‡é¢„æœŸ - å½“å‰ä¼°è®¡ï¼‰ï¼‰

- **SARSAï¼ˆon policyï¼‰ï¼š**  
  \[
  Q(s,a) \leftarrow Q(s,a) + \alpha \Big[ r + \gamma Q(s',a') - Q(s,a) \Big]
  \]
  ï¼ˆç¿»è¯‘ï¼šæ›´æ–°å½“å‰æ™‹å‡åŠ¨ä½œçš„ä»·å€¼ = å½“å‰ä¼°è®¡ + å­¦ä¹ ç‡ Ã—ï¼ˆå³æ—¶å¥–åŠ± + ä¸‹ä¸ªå²—ä½æŒ‰ç°æœ‰ç­–ç•¥é¢„æœŸ - å½“å‰ä¼°è®¡ï¼‰ï¼‰

**æ“ä½œæ­¥éª¤ï¼š**
1. **çŠ¶æ€-åŠ¨ä½œè¯•éªŒ**ï¼šåœ¨æ¯ä¸ªå²—ä½ï¼ˆçŠ¶æ€ï¼‰å°è¯•ä¸åŒæ™‹å‡åŠ¨ä½œï¼ˆä¾‹å¦‚åŠ ç­ã€æ‹é©¬å±ã€è·³æ§½ï¼‰ã€‚
2. **å³æ—¶åé¦ˆä¸æ›´æ–°**ï¼šæ¯ä¸€æ­¥éƒ½ä¾æ®æ”¶åˆ°çš„å³æ—¶å¥–åŠ± \(r\) å’Œä¸‹ä¸€ä¸ªå²—ä½çš„é¢„æœŸä»·å€¼æ›´æ–° \(Q(s,a)\)ã€‚
3. **ç­–ç•¥å¯¼å‡º**ï¼šæœ€ç»ˆå½¢æˆä¸€å¥— \(Q\) å€¼è¡¨ï¼Œæ¯ä¸ªå²—ä½é€‰æ‹©ä½¿ \(Q(s,a)\) æœ€å¤§çš„åŠ¨ä½œå³ä¸ºæœ€ä¼˜æ™‹å‡ç­–ç•¥ã€‚



ä½ å¯ä»¥åªæœ‰ä¸€ä¸ªä»·å€¼å®ä½“$V_\pi$ ,å› ä¸ºå®ƒçš„è¾“å…¥å’ŒçŠ¶æ€ä¸åŠ¨ä½œç›¸å…³(è¿™é‡Œæˆ‘ä»¬ä¸åŒºåˆ†Vå’ŒQï¼Œç•™åˆ°åæ–‡ç»†è¯´) ã€‚è¿™æ„å‘³ç€åªè¦æˆ‘ä»¬çŸ¥é“çŠ¶æ€ç©ºé—´$S$å’ŒåŠ¨ä½œç©ºé—´$\mathcal{A}$ ,$V_\pi$å°±å¯ä»¥ä½œç”¨åˆ°è¿™ä¸¤ä¸ªç©ºé—´ä¸Šå¸®åŠ©æˆ‘ä»¬è¡¡é‡å“ªä¸ªçŠ¶æ€/åŠ¨ä½œçš„ä»·å€¼æœ€å¤§ï¼Œè¿›è€Œéšå¼åœ°æ‰¿æ‹…èµ·åˆ¶å®šç­–ç•¥çš„è§’è‰²ï¼Œæˆ‘ä»¬ä¹Ÿç®¡è¿™ç§æ–¹æ³•å«**value-based**ã€‚
ä½ å¯ä»¥åªæœ‰ä¸€ä¸ªç­–ç•¥å®ä½“$\pi$ ,åœ¨å¯¹ç­–ç•¥çš„ä»·å€¼è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è®©ç­–ç•¥å’Œç¯å¢ƒäº¤äº’å¤šæ¬¡ï¼Œé‡‡æ ·è¶³å¤Ÿå¤šçš„è½¨è¿¹æ•°æ®ï¼Œç”¨è¿™äº›æ•°æ®å»å¯¹ç­–ç•¥çš„ä»·å€¼åšè¯„ä¼°ï¼Œç„¶åå†æ®æ­¤å†³å®šç­–ç•¥çš„è¿­ä»£æ–¹å‘ï¼Œæˆ‘ä»¬ä¹Ÿç®¡è¿™ç§æ–¹æ³•å«**policy-based**ã€‚
ä½ å¯ä»¥åŒæ—¶æœ‰ä»·å€¼å®ä½“$V_\pi$ å’Œç­–ç•¥å®ä½“ $\pi$ ,ç„¶åæŒ‰ç…§ä¸Šé¢è¯´çš„è¿‡ç¨‹è¿›è¡Œè¿­ä»£ï¼Œæˆ‘ä»¬ä¹Ÿç®¡è¿™ç§æ–¹
æ³•å«**actor-critic**,å…¶ä¸­actorè¡¨ç¤ºç­–ç•¥ï¼Œcriticè¡¨ç¤ºä»·å€¼ã€‚è¿™æ˜¯æˆ‘ä»¬æœ¬æ–‡è®¨è®ºçš„é‡ç‚¹ã€‚

---

$$
\pi^g(s)=\arg\max_a\{Q(s,a)\},\forall s\in\mathcal{S} \\
J(\theta)=\mathbb{E}_{s\sim d(s)}\{Q(s,\pi_\theta(s))\} \\
\theta\leftarrow\theta+\beta\nabla_\theta J(\theta) \\
J(\theta)=\mathbb{E}_{s\sim d(s)}\left\{\sum_a\pi_\theta(a|s)Q(s,a)\right\}\\
\nabla_\theta J(\theta)=\sum_sd(s)\sum_a\nabla_\theta\pi_\theta(a|s)Q(s,a)\\
\begin{aligned}\nabla_{\theta}J(\theta)&=\sum_sd_\pi(s)\sum_a\nabla_\theta\pi_\theta(a|s)Q(s,a)\\&=\sum_sd_\pi(s)\sum_a\pi_\theta(a|s)\nabla_\theta\log\pi_\theta(a|s)Q(s,a)\\&=\mathbb{E}_{s\sim d_{\pi},a\sim\pi}\{\nabla_{\theta}\log\pi_{\theta}(a|s)Q(s,a)\},\end{aligned}
$$




Value functionï¼š
é¦–å…ˆæ¯‹åº¸ç½®ç–‘ä¼˜åŒ–ä¸‹é¢function
$$
J_1=\sum_{i=1}^n\left(\hat{v}(s_i,w)-v_\pi(s_i)\right)^2=\sum_{i=1}^n\left(\phi^T(s_i)w-v_\pi(s_i)\right)^2
$$
Two way of expectation:
$$
J(w)=\frac{1}{n}\sum_{s\in\mathcal{S}}(v_\pi(s)-\hat{v}(s,w))^2 \\
J(w)=\sum_{s\in\mathcal{S}}d_\pi(s)(v_\pi(s)-\hat{v}(s,w))^2
$$
$d_\pi(s)$ æ˜¯stationary distribution
Then what can be used for the $\hat{v}(s,w)$? First it can be the 
monte carlo estimation, where we originally have that 

$$
V^{\pi}(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0=s\right] = \mathbb{E}_{\pi}\left[r_0 + \sum_{t=1}^{\infty} \gamma^t r_t \mid s_0=s\right]
$$
Another esitmation for the value function is the TD(0) estimation, where we have that $r_{t+1}+\gamma\hat{v}(s_{t+1},w_{t})$.

é‚£ä¹ˆèƒ½çœ‹åˆ°action-value functionçš„ä¼°è®¡æ–¹æ³•å®é™…ä¸Šä¹Ÿæ˜¯ä¸€æ¨¡ä¸€æ ·çš„
æ¯”å¦‚ä¸‹é¢çš„sarsaå°±æ˜¯è¿™ä¸ªç±»ä¼¼TD(0)
$$
w_{t+1}=w_t+\alpha_t\left[r_{t+1}+\gamma\hat{q}(s_{t+1},a_{t+1},w_t)-\hat{q}(s_t,a_t,w_t)\right]\nabla_w\hat{q}(s_t,a_t,w_t)
$$

ç„¶åQæœ‰å¦‚ä¸‹æ€§è´¨å˜›ï¼š

$$
Q^*(s,a) = \mathbb{E}\left[ r + \gamma V^*(s') \right]
$$
$$
V^*(s') = \max_{a'} Q^*(s',a')
$$
é‚£ä¹ˆä¸æ˜¯å°±æœ‰äº†è¿™ä¸ª
$$
Q^*(s,a) = \mathbb{E}\left[ r + \gamma \max_{a'} Q^*(s',a') \right]
$$
å°±æ˜¯Q learning å˜›
$$
w_{t+1}=w_t+\alpha_t\left[r_{t+1}+\gamma\max_{a\in\mathcal{A}(s_{t+1})}\hat{q}(s_{t+1},a,w_t)-\hat{q}(s_t,a_t,w_t)\right]\nabla_w\hat{q}(s_t,a_t,w_t)
$$


## å¯¹äºç­–ç•¥ä¼˜åŒ–
ç­–ç•¥ä¼˜åŒ–ç›®çš„å°±æ˜¯æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥å§
å’±ä»¬æœ‰å¾ˆå¤šmatric
### Metric 1: Average value
$$
\bar{v}_\pi=\mathbb{E}_{S\sim d}[v_\pi(S)]
$$
æ‰¾åˆ°policyä½¿matricæœ€å¤§
æœ‰çš„äººå¯èƒ½å¸Œæœ›çœ‹åˆ°çš„æ˜¯å¦‚ä¸‹è¿™ä¸ªï¼Œä½†æ˜¯å®é™…ä¸Šä¸€æ ·çš„å˜›
$$
J(\theta)=\lim_{n\to\infty}\mathbb{E}\left[\sum_{t=0}^n\gamma^tR_{t+1}\right]=\mathbb{E}\left[\sum_{t=0}^\infty\gamma^tR_{t+1}\right]
$$
$$
V^{\pi}(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0=s\right] = \mathbb{E}_{\pi}\left[r_0 + \sum_{t=1}^{\infty} \gamma^t r_t \mid s_0=s\right]
$$

### Metric 2: Average reward

$$
\begin{aligned}\bar{r}_{\pi}&\doteq\sum_{s\in\mathcal{S}}d_\pi(s)r_\pi(s)\\&=\mathbb{E}_{S\sim d_\pi}[r_\pi(S)],\end{aligned}
$$

$$
r_\pi(s)\doteq\sum_{a\in\mathcal{A}}\pi(a|s,\theta)r(s,a)=\mathbb{E}_{A\sim\pi(s,\theta)}[r(s,A)|s]
$$
A common metric that readers may often see in the literature is

$$
J(\theta)=\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\left[\sum_{t=0}^{n-1}R_{t+1}\right]
$$
$$
\begin{array}{c|c|c|c}\mathrm{Metric}&\text{Expression 1}&\text{Expression 2}&\text{Expression 3}\\\bar{v}_\pi&\sum_{s\in\mathcal{S}}d(s)v_\pi(s)&\mathbb{E}_{S\sim d}[v_\pi(S)]&\lim_{n\to\infty}\mathbb{E}\left[\sum_{t=0}^n\gamma^tR_{t+1}\right]\\\bar{r}_\pi&\sum_{s\in\mathcal{S}}d_\pi(s)r_\pi(s)&\mathbb{E}_{S\sim d_\pi}[r_\pi(S)]&\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\left[\sum_{t=0}^{n-1}R_{t+1}\right]\end{array}
$$


The gradient in the discounted case 
first we have that 
$$
\begin{aligned}v_\pi(s)&=\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots|S_t=s],\\q_\pi(s,a)&=\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots|S_t=s,A_t=a]\end{aligned}
$$
$$
\text{First, we show that }\bar{v}_{\pi}(\theta)\mathrm{~and~}\bar{r}_{\pi}(\theta)\text{ are equivalent metrics.}
$$
here it comes that 
$$
\bar{r}_\pi=(1-\gamma)\bar{v}_\pi
$$
After some calculation we have that
$$
\begin{aligned}\nabla_\theta\bar{r}_\pi=(1-\gamma)\nabla_\theta\bar{v}_\pi&\approx\sum_{s\in\mathcal{S}}d_\pi(s)\sum_{a\in\mathcal{A}}\nabla_\theta\pi(a|s,\theta)q_\pi(s,a)\\&=\mathbb{E}\left[\nabla_\theta\ln\pi(A|S,\theta)q_\pi(S,A)\right],\end{aligned}
$$
We maximxize the gradient of the metric with respect to the policy parameters.
$$
\nabla_\theta \pi(a \mid s)=\pi(a \mid s) \nabla_\theta \log \pi(a \mid s)
$$
$$
\begin{aligned}\theta_{t+1}&=\theta_t+\alpha\nabla_\theta J(\theta_t)\\&=\theta_t+\alpha\mathbb{E}\left[\nabla_\theta\ln\pi(A|S,\theta_t)q_\pi(S,A)\right],\end{aligned}
$$
Also can be written as the 
$$
\theta_{t+1}=\theta_t+\alpha\nabla_\theta\ln\pi(a_t|s_t,\theta_t)q_t(s_t,a_t)
$$
If we use Monte Carlo esitmation for the $q_t(s_t,a_t)$, then it is call 
the REINFORCE method.


### Baseline invariance
$$
\mathbb{E}_{S\sim\eta,A\sim\pi}\left[\nabla_\theta\ln\pi(A|S,\theta_t)q_\pi(S,A)\right]=\mathbb{E}_{S\sim\eta,A\sim\pi}\left[\nabla_\theta\ln\pi(A|S,\theta_t)(q_\pi(S,A)-b(S))\right]
$$
Let the $b(S) = v_\pi(S)$ is actually an suboptimal solution that can decrease the variance of the gradient.

Off policy 
importance sampling
$$
\nabla_\theta J(\theta)=\mathbb{E}\left[\frac{\pi(A|S,\theta)}{\beta(A|S)}\nabla_\theta\ln\pi(A|S,\theta)\left(q_\pi(S,A)-v_\pi(S)\right)\right]
$$
And noticing that the $\beta (A|S)$ is the behavior policy


### Deterministic actor-Critic
This section shows that deterministic policies can also be used in policy gradient methods. Here, â€œdeterministicâ€ indicates that, for any state, a single action is given a probability of one and all the other actions are given probabilities of zero.

Then the gradient become

$$
\begin{aligned}\nabla_{\theta}J(\theta)&=\sum_{s\in\mathcal{S}}\eta(s)\nabla_\theta\mu(s)\left(\nabla_aq_\mu(s,a)\right)|_{a=\mu(s)}\\&=\mathbb{E}_{S\sim\eta}\left[\nabla_\theta\mu(S)\left(\nabla_aq_\mu(S,a)\right)|_{a=\mu(S)}\right]\end{aligned}
$$
# ä»ç­–ç•¥æ¢¯åº¦åˆ°PPOçš„æ¨å¯¼

---

## 1. ç­–ç•¥æ¢¯åº¦åŸºç¡€
**ç›®æ ‡**ï¼šé€šè¿‡è°ƒæ•´ç­–ç•¥å‚æ•°$\theta$ï¼Œæœ€å¤§åŒ–é•¿æœŸå¥–åŠ±$J(\theta)$ã€‚  
**æ ¸å¿ƒå…¬å¼**ï¼š
\[
\theta_{t+1} = \theta_t + \alpha \cdot \mathbb{E}\left[\nabla_\theta \ln \pi(A|S,\theta) \cdot Q(S,A)\right]
\]
- $\nabla_\theta \ln \pi(A|S,\theta)$ï¼šç­–ç•¥é€‰æ‹©åŠ¨ä½œçš„â€œå€¾å‘æ€§â€æ¢¯åº¦ã€‚
- $Q(S,A)$ï¼šåŠ¨ä½œä»·å€¼ï¼ŒæŒ‡å¯¼æ¢¯åº¦æ›´æ–°æ–¹å‘ã€‚

**é—®é¢˜**ï¼šç›´æ¥ä½¿ç”¨ç­–ç•¥æ¢¯åº¦å®¹æ˜“å¯¼è‡´é«˜æ–¹å·®ï¼Œå°¤å…¶åœ¨ç¦»ç­–ç•¥ï¼ˆOff-Policyï¼‰åœºæ™¯ä¸‹ã€‚

---

## 2. é‡è¦æ€§é‡‡æ ·ä¸åŸºçº¿ä¸å˜æ€§
### é‡è¦æ€§é‡‡æ ·ä¿®æ­£
å½“æ•°æ®æ¥è‡ªæ—§ç­–ç•¥$\beta$æ—¶ï¼Œéœ€å¼•å…¥é‡è¦æ€§æ¯”ç‡ï¼š
\[
\nabla_\theta J(\theta) = \mathbb{E}\left[\frac{\pi(A|S,\theta)}{\beta(A|S)} \cdot \nabla_\theta \ln \pi(A|S,\theta) \cdot (Q(S,A) - V(S))\right]
\]
- $\frac{\pi}{\beta}$ï¼šä¿®æ­£æ–°æ—§ç­–ç•¥å·®å¼‚çš„æƒé‡ã€‚
- $(Q - V)$ï¼šç”¨ä¼˜åŠ¿å‡½æ•°$A(S,A)$æ›¿ä»£ï¼Œå‡å°‘æ–¹å·®ã€‚

### åŸºçº¿ä¸å˜æ€§ï¼ˆBaseline Invarianceï¼‰
æ·»åŠ åŸºçº¿$b(S)=V(S)$åï¼Œæ¢¯åº¦ä¸å˜ä½†æ–¹å·®é™ä½ï¼š
\[
\mathbb{E}\left[\nabla_\theta \ln \pi(A|S,\theta) \cdot (Q(S,A) - V(S))\right]
\]

---

## 3. TRPOï¼šä¿¡ä»»åŒºåŸŸä¼˜åŒ–
**æ ¸å¿ƒæ€æƒ³**ï¼šé™åˆ¶ç­–ç•¥æ›´æ–°çš„å¹…åº¦ï¼Œé¿å…â€œè·¨æ­¥è¿‡å¤§â€ï¼š
\[
\mathbb{E}[KL(\beta \| \pi_{\text{new}})] \leq \delta
\]
- **KLæ•£åº¦çº¦æŸ**ï¼šä¿è¯æ–°æ—§ç­–ç•¥å·®å¼‚åœ¨$\delta$èŒƒå›´å†…ã€‚
- **é—®é¢˜**ï¼šéœ€è¦è®¡ç®—å¤æ‚çš„äºŒé˜¶å¯¼æ•°ï¼ˆHessiançŸ©é˜µï¼‰ï¼Œå®ç°å›°éš¾ã€‚

---

## 4. PPOï¼šè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–
### æ ¸å¿ƒæ”¹è¿›ï¼šå‰ªåˆ‡ï¼ˆClippingï¼‰
ç›´æ¥é™åˆ¶é‡è¦æ€§æ¯”ç‡$r(\theta)=\frac{\pi}{\pi_{\text{old}}}$çš„èŒƒå›´ï¼š
\[
L(\theta) = \mathbb{E}\left[ \min\left( r(\theta) \cdot A, \text{clip}(r(\theta), 1-\epsilon, 1+\epsilon) \cdot A \right) \right]
\]
- **å‰ªåˆ‡æ“ä½œ**ï¼šå°†$r(\theta)$é™åˆ¶åœ¨$[1-\epsilon, 1+\epsilon]$ä¹‹é—´ï¼ˆå¦‚$\epsilon=0.2$ï¼‰ã€‚
- **å–æœ€å°å€¼**ï¼šé˜²æ­¢ç­–ç•¥åœ¨ä¼˜åŠ¿å‡½æ•°æ–¹å‘ç›¸åæ—¶è¿‡åº¦æ›´æ–°ã€‚

### ç›´è§‚è§£é‡Š
- **æ¯”å–»**ï¼šç»™ç­–ç•¥æ›´æ–°è£…ä¸€ä¸ªâ€œç¼“å†²å™¨â€ï¼Œé¿å…å‰§çƒˆéœ‡è¡ã€‚
- **ä¼˜åŠ¿**ï¼šæ— éœ€è®¡ç®—KLæ•£åº¦ï¼Œå®ç°ç®€å•ä¸”ç¨³å®šã€‚

---

## 5. PPOçš„å®ç°æ­¥éª¤
1. **æ•°æ®æ”¶é›†**ï¼šç”¨å½“å‰ç­–ç•¥$\pi_{\text{old}}$ä¸ç¯å¢ƒäº¤äº’ï¼Œç”Ÿæˆè½¨è¿¹ã€‚
2. **è®¡ç®—ä¼˜åŠ¿ä¼°è®¡**ï¼šä½¿ç”¨å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰è®¡ç®—$A_t$ã€‚
3. **ä¼˜åŒ–ç›®æ ‡å‡½æ•°**ï¼š
   - å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ã€‚
   - å¤šæ¬¡è¿­ä»£æ›´æ–°ç­–ç•¥ï¼ˆé€šå¸¸3-10æ¬¡ï¼‰ã€‚
4. **æ›´æ–°æ—§ç­–ç•¥**ï¼šæ¯éš”$K$æ­¥åŒæ­¥$\pi_{\text{old}} \leftarrow \pi$ã€‚

---

## 6. PPOçš„å˜ä½“
### è‡ªé€‚åº”KLæƒ©ç½š
\[
L(\theta) = \mathbb{E}\left[ r(\theta) \cdot A - \beta \cdot KL(\pi_{\text{old}}, \pi) \right]
\]
- åŠ¨æ€è°ƒæ•´$\beta$ä»¥ç»´æŒKLæ•£åº¦æ¥è¿‘ç›®æ ‡å€¼ã€‚
- **ç¼ºç‚¹**ï¼šè°ƒå‚å¤æ‚ï¼Œå®è·µä¸­è¾ƒå°‘ä½¿ç”¨ã€‚

---

## 7. æ€»ç»“ï¼šPPOçš„ä¼˜åŠ¿
| é—®é¢˜                | PPOçš„è§£å†³æ–¹æ¡ˆ               |
|---------------------|---------------------------|
| é«˜æ–¹å·®              | å‰ªåˆ‡é‡è¦æ€§æ¯”ç‡ + ä¼˜åŠ¿å‡½æ•°    |
| æ›´æ–°ä¸ç¨³å®š          | ä¿¡ä»»åŒºåŸŸæ€æƒ³ï¼ˆéšå¼çº¦æŸï¼‰     |
| å®ç°å¤æ‚ï¼ˆå¦‚TRPOï¼‰  | ä¸€é˜¶ä¼˜åŒ–ï¼Œæ— éœ€äºŒé˜¶å¯¼æ•°       |




**æ ¸å¿ƒä»·å€¼**ï¼šåœ¨ç®€å•æ€§å’Œç¨³å®šæ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œæˆä¸ºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ ‡æ†ç®—æ³•ã€‚




## DPO
### Reward Modeling Phase

è¿™ä¸€é˜¶æ®µçš„ç›®çš„æ˜¯è®­ç»ƒä¸€ä¸ªReward Modelï¼Œå¯¹äººç±»åå¥½è¿›è¡Œå»ºæ¨¡ã€‚æ­£å¦‚ä¸Šæ–‡æ‰€è¿°ï¼ŒReward Modelä¼šä¸ºæ¯ä¸ªæ¨¡å‹è¿”å›è¿›è¡Œæ‰“åˆ†ï¼Œè®¡ç®—å…¶rewardï¼Œæˆ‘ä»¬å¸Œæœ›è¿™ä¸ªrewardèƒ½å¤Ÿé€šè¿‡Bradleyâ€“Terry modelæ¥é¢„æµ‹äººç±»åå¥½ã€‚
$$
p^*(y_1\succ y_2\mid x)=\frac{\exp\left(r^*(x,y_1)\right)}{\exp\left(r^*(x,y_1)\right)+\exp\left(r^*(x,y_2)\right)}
$$
è¿™ä¸ªå°±æ˜¯Bradleyâ€“Terry modelï¼Œè¿™é‡Œç”¨çš„æ˜¯æˆ‘ä»¬ç†æƒ³å½“ä¸­å­˜åœ¨çš„é‚£ä¸ªéšå«çš„reward 
 æ¥è®¡ç®—äººç±»åå¥½ã€‚è¿™ä¸ª 
 æ˜¯æˆ‘ä»¬Reward Modelè®­ç»ƒçš„ç›®æ ‡ï¼Œå½“ç„¶å®é™…å½“ä¸­æˆ‘ä»¬åªèƒ½é€šè¿‡è®­ç»ƒæ•°æ®å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¯¹ 
 åšä¸€ä¸ªä¼°è®¡ã€‚è¿™ä¸ªè®­ç»ƒæ•°æ®ä¸€èˆ¬æ¥æºäºSFTä¹‹åçš„æ¨¡å‹è¿”å›çš„ç»“æœï¼Œä¸€èˆ¬è€Œè¨€ä¼šç”¨åŒä¸€ä¸ªprefix 
 è¿”å›ä¸¤ä¸ªç»“æœ ,ç”±äººç±»è¿›è¡Œæ‰“æ ‡ï¼Œé€‰æ‹©äººç±»å–œå¥½çš„ç»“æœï¼Œæ„æˆäººç±»åå¥½æ•°æ®ã€‚

$$
\mathcal{L}_R(r_\phi,\mathcal{D})=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\left[\log\sigma(r_\phi(x,y_w)-r_\phi(x,y_l))\right]
$$



$$
\begin{aligned}\max_\pi\mathbb{E}_{x\sim\mathcal{D},y\sim\pi}&\begin{bmatrix}r(x,y)\end{bmatrix}-\beta\mathbb{D}_{\mathrm{KL}}\begin{bmatrix}\pi(y|x)&\mid\mid\pi_{\mathrm{ref}}(y|x)\end{bmatrix}\\&=\max_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[r(x,y)-\beta\log\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)}\right]\\&=\min_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)}-\frac{1}{\beta}r(x,y)\right]\\&=\min_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)}-\log Z(x)\right]\end{aligned}
$$

where we partition function:
$$
Z(x)=\sum_y\pi_{\mathrm{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)
$$
ç„¶åæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ
$$
\pi^*(y|x)=\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)
$$
Ï€*æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå› ä¸ºå®ƒä¸€å®šå¤§äºç­‰äº0ï¼Œä¸”å¯¹æ‰€æœ‰çš„yæ±‚å’Œç­‰äº1ã€‚äºæ˜¯ä¸Šé¢çš„å¼å­å¯ä»¥è¡¨ç¤ºä¸º



$$
\min_\pi\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi^*(y|x)}\right]-\log Z(x)\right]=\min_\pi\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{D}_{\mathrm{KL}}(\pi(y|x)\parallel\pi^*(y|x))_\text{ä¸€}{\log Z(x)}\right]
$$
æˆ‘ä»¬çŸ¥é“å¯¹äºKLé¡¹ï¼Œå½“ä¸”ä»…å½“Ï€=Ï€*æ—¶ï¼ŒKLé¡¹å–åˆ°æœ€å°å€¼0ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥å¾—åˆ°æœ€ä¼˜LMçš„æ˜¾å¼è¡¨è¾¾



è¿™ä¸ªæ˜¾å¼è¡¨è¾¾ä¾èµ–äºreference modelå’Œreward modelï¼Œåœ¨å®é™…å½“ä¸­æ˜¯æ²¡æœ‰åŠæ³•è®¡ç®—çš„ï¼Œå› ä¸ºZéœ€è¦æŠŠæ‰€æœ‰å¯èƒ½çš„yéƒ½ç®—ä¸€éï¼Œè¿™ä¸ªæ˜¯åšä¸åˆ°çš„ã€‚

æˆ‘ä»¬å¯¹è¿™ä¸ªæ˜¾å¼è¡¨è¾¾ä¸¤è¾¹å–logï¼Œé€šè¿‡ä¸€äº›ç®€å•çš„å˜æ¢ï¼Œå¯ä»¥å¾—åˆ°

$$
r(x, y)=\beta \log \frac{\pi_r(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}+\beta \log Z(x) .
$$

äºæ˜¯æˆ‘ä»¬å¾—åˆ°äº†reward model $r(x,y)$
çš„ä¸€ä¸ªæ˜¾å¼è¡¨è¾¾ã€‚æˆ‘ä»¬å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼ŒæŠŠè¿™ä¸ªrå¸¦å…¥åˆ°ä¹‹å‰æåˆ°çš„ç”¨Bradleyâ€“Terry modelå»ºæ¨¡çš„äººç±»åå¥½æ¦‚ç‡æ¨¡å‹é‡Œé¢
$$
p^*\left(y_1 \succ y_2 \mid x\right)=\frac{1}{1+\exp \left(\beta \log \frac{\pi^*\left(y_2 \mid x\right)}{\pi_{\text {ref }}\left(y_2 \mid x\right)}-\beta \log \frac{\pi^*\left(y_1 \mid x\right)}{\pi_{\text {ref }}\left(y_1 \mid x\right)}\right)}
$$

ç„¶åæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œç”±äºè¿™ä¸ªæ¨¡å‹è®¡ç®—æ¦‚ç‡çš„æ—¶å€™åªå…³å¿ƒä¸¤ä¸ªæ ·æœ¬rewardçš„å·®å€¼ï¼Œå› æ­¤è¿™é‡Œçš„logZ(x)é¡¹è¢«æŠµæ¶ˆäº†ï¼äºæ˜¯æˆ‘ä»¬å¯ä»¥è½¬è€Œç”¨MLEç›´æ¥åœ¨è¿™ä¸ªæ¦‚ç‡æ¨¡å‹ä¸Šç›´æ¥ä¼˜åŒ–LMï¼Œå»å¾—åˆ°æˆ‘ä»¬å¸Œæœ›çš„æœ€ä¼˜çš„Ï€*ã€‚
$$
\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\right)\right]
$$


# PPO å…·ä½“ç»†èŠ‚
$$
\mathcal{J}_{\text{PPO}}(\theta) = \mathbb{E}_{t} \left[ 
\min \left( 
r_t(\theta) \cdot A_t,\ 
\text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \cdot A_t 
\right) 
\right]
$$



![avatar](PPO.png)


PPO éœ€è¦train ä¸€ä¸ª Value model å»è®¡ç®— Advantage è€Œ GRPO çœå»äº†è¿™ä¸ªä¸œè¥¿ã€‚
$$
\begin{aligned}
\mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E} \Bigg[ 
& q \sim P(Q),\ \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O \mid q) \Bigg] \\
& \cdot \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} 
\min \Bigg( 
r_{i,t}(\theta) \cdot \hat{A}_{i,t},\ 
\text{clip}(r_{i,t}(\theta), 1 - \varepsilon, 1 + \varepsilon) \cdot \hat{A}_{i,t} 
\Bigg) \\
& - \beta \cdot \mathbb{D}_{\mathrm{KL}} \left[ \pi_\theta \,\|\, \pi_{\text{ref}} \right]
\end{aligned}
$$

GRPO set 
$$
\hat{A}_{i,t}=\widetilde{r}_{i}=\frac{r_{i}-\mathrm{mean}(\mathbf{r})}{\mathrm{std}(\mathbf{r})}
$$
t æ˜¯æŒ‡æ¯ä¸ªtokençº§åˆ«

reward model åœ¨ R1 é‡Œé¢å°±æ˜¯ç®€å•çš„rubic model ä½†è¿™deepseek mathé‡Œé¢æ˜¯learn çš„ model

![avatar](image.png)


ä»€ä¹ˆæ˜¯ LLM ä¸­çš„ RLå¦‚æœæˆ‘ä»¬ä» loss å‡½æ•°çš„è§’åº¦æ¥çœ‹ sft å’Œ rlhfï¼Œä¼šå‘ç°äºŒè€…åœ¨æœ¬è´¨ä¸Šæ²¡æœ‰å·®åˆ«ï¼šæ— ééƒ½æ˜¯ä¸€ä¸ªæ¡ä»¶æ¦‚ç‡å…¬å¼å˜›ï¼Œå›´ç»•ç€ next_token çš„ probability åšæ–‡ç« ã€‚åªä¸è¿‡åœ¨å®ç°ç»†èŠ‚ä¸Šï¼Œsft çš„ next_token æœ‰ä¸€ä¸ªæ˜ç¡®çš„ targetï¼Œè·ç¦»è¿™ä¸ª target è¿œ loss å°±å¤§ï¼Œå¦åˆ™ loss å°±å°ï¼›rlhf çš„ next_token åˆ™æ˜¯æœ‰ä¸€ä¸ª rewardï¼Œå¦‚æœè¿™ä¸ª reward é«˜å°±é¼“åŠ±å®ƒï¼Œreward ä½å°±æ‰“å‹å®ƒã€‚
é‚£ä¹ˆï¼Œæ—¢ç„¶ä¸¤ç§ç®—æ³•åœ¨ loss å‡½æ•°ä¸Šæ²¡æœ‰æœ¬è´¨åŒºåˆ«ï¼Œä»–ä»¬çš„åŒºåˆ«åˆä½“ç°åœ¨å“ªé‡Œå‘¢ï¼Ÿæˆ‘ä¸ªäººçš„è§‚ç‚¹æ˜¯ï¼šexploreã€‚è¿™ä¹Ÿæ˜¯æˆ‘å¯¹å¼ºåŒ–å­¦ä¹ çš„ç†è§£ï¼šâ€œè‡ªå·±ç©ï¼Œæ—äººæ¥çº æ­£â€ã€‚
post-training é˜¶æ®µçš„æ‰€æœ‰ç®—æ³•éƒ½åœ¨åšä¸€ä»¶äº‹ï¼šè¾“å‡ºå½“å‰æ–‡æœ¬ä¸‹çš„ next_tokenï¼Œç„¶åçº é”™ã€‚åªä¸è¿‡ sft åœ¨å¼ºåˆ¶å­¦ï¼Œrlhf åœ¨ explore å­¦ï¼Œå¼ºåˆ¶å­¦è¿›æ­¥å¿«ï¼Œexplore å­¦æ ¹åŸºç¨³ã€‚

post training ç®—æ³•çš„ç»Ÿä¸€å»ºæ¨¡deepseek åœ¨å»å¹´çš„æ—¶å€™ï¼Œå°±å·²ç»åœ¨æŠ€æœ¯æŠ¥å‘Šé‡ŒæŒ‡å‡ºè¿‡ï¼Œsft å’Œ rlhf ç®—æ³•åœ¨ loss å‡½æ•°çš„è®¾è®¡ä¸Šæ²¡æœ‰æœ¬è´¨åŒºåˆ«ã€‚å…·ä½“æ¥è¯´ï¼Œdeepseek è®¤ä¸º post training ç®—æ³•åŒ…æ‹¬ä¸‰è¦ç´ ï¼šå¯åŠ¨æ•°æ®ï¼Œreward functionï¼Œtoken ç²’åº¦çš„ gradient coefficientã€‚sft çš„ Gradient Coefficient æ˜¯ 1ï¼Œppo çš„ Gradient Coefficient æ˜¯ Advantageã€‚

From DeepSeekMath
| Methods | Data Source | Reward Function | Gradient Coefficient |
| :--- | :---: | :---: | :---: |
| SFT | $q, o \sim P_{s f t}(Q, O)$ | - | 1 |
| RFT | $q \sim P_{s f t}(Q), o \sim \pi_{s f t}(O \mid q)$ | Rule | Equation 10 |
| DPO | $q \sim P_{s f t}(Q), o^{+}, o^{-} \sim \pi_{s f t}(O \mid q)$ | Rule | Equation 14 |
| Online RFT | $q \sim P_{s f t}(Q), o \sim \pi_\theta(O \mid q)$ | Rule | Equation 10 |
| PPO | $q \sim P_{s f t}(Q), o \sim \pi_\theta(O \mid q)$ | Model | Equation 18 |
| GRPO | $q \sim P_{s f t}(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_\theta(O \mid q)$ | Model | Equation 21 |